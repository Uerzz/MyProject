{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation time 3599\n",
      "the number of time interval: 6\n",
      "time interval: 600\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import six.moves.cPickle as pickle\n",
    "#from model import config\n",
    "import config_weibo_1 as config\n",
    "import networkx as nx\n",
    "import scipy.sparse\n",
    "import gc\n",
    "import math\n",
    "LABEL_NUM = 0\n",
    "n_time_interval= config.n_time_interval\n",
    "time_interval = config.time_interval\n",
    "degree_interval_list= config.degree_interval_list\n",
    "n_degree_interval=len(degree_interval_list)+1\n",
    "n_pre_degree = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans the original ids to 1~n\n",
    "class IndexDict:\n",
    "    def __init__(self, original_ids):\n",
    "        self.original_to_new = {}\n",
    "        self.new_to_original = []\n",
    "        cnt = 0\n",
    "        for i in original_ids:\n",
    "            new = self.original_to_new.get(i, cnt)\n",
    "            if new == cnt:\n",
    "                self.original_to_new[i] = cnt\n",
    "                cnt += 1\n",
    "                self.new_to_original.append(i)\n",
    "\n",
    "    def new(self, original):\n",
    "        if type(original) is int:\n",
    "            return self.original_to_new[original]\n",
    "        else:\n",
    "            if type(original[0]) is int:\n",
    "                return [self.original_to_new[i] for i in original]\n",
    "            else:\n",
    "                return [[self.original_to_new[i] for i in l] for l in original]\n",
    "\n",
    "    def original(self, new):\n",
    "        if type(new) is int:\n",
    "            return self.new_to_original[new]\n",
    "        else:\n",
    "            if type(new[0]) is int:\n",
    "                return [self.new_to_original[i] for i in new]\n",
    "            else:\n",
    "                return [[self.new_to_original[i] for i in l] for l in new]\n",
    "\n",
    "    def length(self):\n",
    "        return len(self.new_to_original)\n",
    "\n",
    "#trainsform the sequence to list\n",
    "def sequence2list(flename):\n",
    "    graphs = {}\n",
    "    with open(flename, 'r') as f:\n",
    "        for line in f:\n",
    "            walks = line.strip().split('\\t')\n",
    "            graphs[walks[0]] = [] #walk[0] = cascadeID\n",
    "            for i in range(1, len(walks)):\n",
    "                s = walks[i].split(\":\")[0] #node\n",
    "                t = walks[i].split(\":\")[1] #time\n",
    "                graphs[walks[0]].append([[str(xx) for xx in s.split(\",\")],int(t)])\n",
    "    return graphs\n",
    "\n",
    "#read label and size from cascade file\n",
    "def read_labelANDsize(filename):\n",
    "    labels = {}\n",
    "    sizes = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            profile = line.split('\\t')\n",
    "            labels[profile[0]] = profile[-1]\n",
    "            sizes[profile[0]] = int(profile[3])\n",
    "    return labels,sizes\n",
    "\n",
    "def get_original_ids(graphs):\n",
    "    original_ids = set()\n",
    "    for graph in graphs.keys():\n",
    "        for walk in graphs[graph]:\n",
    "            for i in walk[0]:\n",
    "                original_ids.add(i)\n",
    "    print (\"length of original isd:\",len(original_ids))\n",
    "    return original_ids\n",
    "\n",
    "def get_nodes(graph):\n",
    "    nodes = {}\n",
    "    j = 0\n",
    "    for walk in graph:\n",
    "        for i in walk[0]:\n",
    "            if i not in nodes.keys():\n",
    "                nodes[i] = j\n",
    "                j = j+1\n",
    "    return nodes\n",
    "def project(degree_dis_Tmp,degree_interval_list):#将degree_dis_Tmp映射到degree_interval_list区间内\n",
    "    degree_his=np.zeros(shape=(1, len(degree_interval_list)+1))\n",
    "    j_begin = 0\n",
    "    for k in range(len(degree_interval_list)):\n",
    "        for j in range (j_begin,len(degree_dis_Tmp)):\n",
    "            if j<= degree_interval_list[k]:\n",
    "                degree_his[0,k] += degree_dis_Tmp[j]\n",
    "                j_begin +=1\n",
    "            else:\n",
    "                break\n",
    "    if j_begin<len(degree_dis_Tmp):\n",
    "        for j in range (j_begin,len(degree_dis_Tmp)):\n",
    "            degree_his[0,-1] += degree_dis_Tmp[j]\n",
    "    return degree_his\n",
    "        \n",
    "def write_XYSIZE_data(graphs,labels,sizes,LEN_SEQUENCE,NUM_SEQUENCE,index,max_num, filename):\n",
    "    #get the x,y,and size  data\n",
    "    id_data = []\n",
    "    pre_x_data = []\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    sz_data = []\n",
    "    time_data = []\n",
    "    Laplacian_data = []\n",
    "    sizeList =[]\n",
    "    for key,graph in graphs.items():\n",
    "        id = key\n",
    "        label = labels[key].split()\n",
    "        y = int(label[LABEL_NUM]) #label\n",
    "        temp_time = [] #store time\n",
    "        size_temp = len(graph)\n",
    "        if size_temp !=  sizes[key]:\n",
    "            print (size_temp,sizes[key])\n",
    "        nodes_items = get_nodes(graph)\n",
    "        nodes_list = nodes_items.values()\n",
    "        nx_G = nx.DiGraph()\n",
    "        nx_G.add_nodes_from(nodes_list)\n",
    "        sizeList.append(len(nodes_list))\n",
    "        graph = sorted(graph,key=(lambda x:x[1]))## 没有按时间排序？\n",
    "        tmp_degree=np.zeros(shape=(n_time_interval, n_degree_interval))\n",
    "        pre_degree=np.zeros(shape=(n_time_interval, n_pre_degree))\n",
    "        k=0\n",
    "        for walk in graph:\n",
    "            walk_time = walk[1] ## 没有按时间排序？\n",
    "            #print (walk_time)\n",
    "            temp_time.append(walk_time)\n",
    "            k_new = int(math.floor(walk_time/ time_interval))\n",
    "            if k_new > k:## degree distribution\n",
    "                degree_dis_Tmp=nx.degree_histogram(nx_G)\n",
    "                tmp_degree[k]= project(degree_dis_Tmp,degree_interval_list)#转换成 n_degree_interval个数\n",
    "            k = k_new\n",
    "\n",
    "            if walk_time == 0:\n",
    "                nx_G.add_edge(nodes_items.get(walk[0][0]), nodes_items.get(walk[0][0]))\n",
    "            for i in range(len(walk[0])-1):\n",
    "                nx_G.add_edge(nodes_items.get(walk[0][i]),nodes_items.get(walk[0][i+1]))\n",
    "        for kth in range(k,n_time_interval):\n",
    "            degree_dis_Tmp = nx.degree_histogram(nx_G)\n",
    "            tmp_edges = nx_G.number_of_edges()\n",
    "            tmp_nodes = nx_G.number_of_nodes()\n",
    "            tmp_density = nx.density(nx_G)\n",
    "            tmp_degree_centrality = np.average(list(nx.degree_centrality(nx_G).values()))\n",
    "            tmp_closeness = np.average(list(nx.closeness_centrality(nx_G).values()))\n",
    "            tmp_betweens = np.average(list(nx.betweenness_centrality(nx_G).values()))\n",
    "            pre_degree[k]= [tmp_edges,tmp_nodes,tmp_density,tmp_degree_centrality,tmp_closeness,tmp_betweens]\n",
    "            tmp_degree[k]= project(degree_dis_Tmp,degree_interval_list)\n",
    "            \n",
    "        #caculate laplacian\n",
    "        Laplacian=[]\n",
    "        time_data.append(temp_time)\n",
    "        id_data.append(id)\n",
    "        pre_x_data.append(pre_degree)\n",
    "        x_data.append(tmp_degree)\n",
    "        y_data.append(np.log(y+1.0)/np.log(2.0))\n",
    "        Laplacian_data.append(Laplacian)\n",
    "        sz_data.append(size_temp)\n",
    "    gc.collect()\n",
    "    print ('ave.size:'+str(sum(sizeList)/len(sizeList)))\n",
    "    pickle.dump((id_data,x_data,pre_x_data,Laplacian_data,y_data, sz_data, time_data,index.length()), open(filename,'wb'))\n",
    "    \n",
    "def get_maxsize(sizes):\n",
    "    max_size = 0\n",
    "    for cascadeID in sizes:\n",
    "        max_size = max(max_size,sizes[cascadeID])\n",
    "    gc.collect()\n",
    "    return max_size\n",
    "\n",
    "def get_max_length(graphs):\n",
    "    len_sequence = 0\n",
    "    max_num = 0\n",
    "    for cascadeID in graphs:\n",
    "        max_num = max(max_num,len(graphs[cascadeID]))\n",
    "        for sequence in graphs[cascadeID]:\n",
    "            len_sequence = max(len_sequence,len(sequence[0]))\n",
    "    gc.collect()\n",
    "    return len_sequence\n",
    "\n",
    "def get_max_node_num(graphs):\n",
    "    max_num = 0\n",
    "    for key,graph in graphs.items():\n",
    "        nodes = get_nodes(graph)\n",
    "        max_num = max(max_num,len(nodes))\n",
    "    return max_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of original isd: 1024085\n",
      "length of original isd: 277447\n",
      "length of original isd: 269944\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    ### data set ###\n",
    "    graphs_train = sequence2list(config.shortestpath_train)\n",
    "    graphs_val = sequence2list(config.shortestpath_val)\n",
    "    graphs_test = sequence2list(config.shortestpath_test)\n",
    "\n",
    "    ## get Laplacian ##\n",
    "    cascade_train = config.cascade_train\n",
    "    cascade_test = config.cascade_test\n",
    "    cascade_val = config.cascade_val\n",
    "\n",
    "    ### get labels ###\n",
    "    labels_train, sizes_train = read_labelANDsize(config.cascade_train)  # 标签以及观测到的长度\n",
    "    labels_val, sizes_val = read_labelANDsize(config.cascade_val)\n",
    "    labels_test, sizes_test = read_labelANDsize(config.cascade_test)\n",
    "    NUM_SEQUENCE = max(get_maxsize(sizes_train),get_maxsize(sizes_val),get_maxsize(sizes_test))# 观测到的最大长度\n",
    "\n",
    "    LEN_SEQUENCE_train = get_max_length(graphs_train) #最大步长\n",
    "    LEN_SEQUENCE_val = get_max_length(graphs_val)\n",
    "    LEN_SEQUENCE_test = get_max_length(graphs_test)\n",
    "    LEN_SEQUENCE = max(LEN_SEQUENCE_train,LEN_SEQUENCE_val,LEN_SEQUENCE_test)\n",
    "\n",
    "    max_num_train = get_max_node_num(graphs_train)\n",
    "    max_num_test = get_max_node_num(graphs_test)\n",
    "    max_num_val = get_max_node_num(graphs_val)\n",
    "    max_num = max(max_num_train, max_num_test, max_num_val)\n",
    "\n",
    "    # get the total original_ids and tranform the index from 0 ~n-1\n",
    "    original_ids = get_original_ids(graphs_train)\\\n",
    "                    .union(get_original_ids(graphs_val))\\\n",
    "                    .union(get_original_ids(graphs_test))\n",
    "\n",
    "    original_ids.add(-1)\n",
    "    ## index is new index\n",
    "    index = IndexDict(original_ids)\n",
    "    \n",
    "    print(\"create train\")\n",
    "    write_XYSIZE_data(graphs_train, labels_train,sizes_train,LEN_SEQUENCE,NUM_SEQUENCE,index,max_num, config.train_pkl)\n",
    "    print(\"create val an test\")\n",
    "    write_XYSIZE_data(graphs_val, labels_val, sizes_val,LEN_SEQUENCE,NUM_SEQUENCE,index,max_num, config.val_pkl)\n",
    "    write_XYSIZE_data(graphs_test, labels_test, sizes_test,LEN_SEQUENCE,NUM_SEQUENCE,index,max_num,config.test_pkl)\n",
    "    pickle.dump((len(original_ids),NUM_SEQUENCE,LEN_SEQUENCE), open(config.information,'wb'))\n",
    "    print(\"Finish!!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
