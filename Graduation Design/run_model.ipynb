{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation time 3599\n",
      "the number of time interval: 6\n",
      "time interval: 600\n"
     ]
    }
   ],
   "source": [
    "# https://blog.csdn.net/marsjhao/article/details/68490105 https://blog.csdn.net/u014281392/article/details/77103747\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "np.random.seed(355)  # for reproducibility\n",
    "from keras.models import Sequential \n",
    "from keras import layers\n",
    "from keras.layers import Dropout, Bidirectional,Activation,Flatten,GRU,LSTM\n",
    "from keras.layers import Dense, Input, TimeDistributed,RepeatVector\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras import Model\n",
    "from keras.optimizers import Adam\n",
    "#from keras.utils import multi_gpu_model\n",
    "import math\n",
    "import time\n",
    "import config_weibo_1 as cf\n",
    "import six.moves.cPickle as pickle\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 10)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 20\n",
    "CELL_SIZE=32\n",
    "OUTPUT_SIZE=20\n",
    "TIME_STEPS = cf.n_time_interval\n",
    "datasetName =cf.datasetName\n",
    "INPUT_SIZE = len(cf.degree_interval_list)+1\n",
    "input_shape = (TIME_STEPS, INPUT_SIZE)\n",
    "PRE_INPUT_SIZE = 6\n",
    "PRE_OUTPUT_SIZE = 12\n",
    "print (input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Z_Score(data):\n",
    "    lenth = len(data)\n",
    "    total = sum(data)\n",
    "    ave = float(total)/lenth\n",
    "    tempsum = sum([pow(data[i] - ave,2) for i in range(lenth)])\n",
    "    tempsum = pow(float(tempsum)/lenth,0.5)\n",
    "    for i in range(lenth):\n",
    "        data[i] = (data[i] - ave)/tempsum\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29529, 6, 10)\n",
      "(29529, 6, 6)\n",
      "(6327, 6, 10)\n",
      "(6327, 6, 6)\n",
      "(6327, 6, 10)\n",
      "(6327, 6, 6)\n",
      "(29529,)\n"
     ]
    }
   ],
   "source": [
    "id_train, x_train,pre_x_train, L, y_train, sz_train, time_train, vocabulary_size = pickle.load(open(cf.train_pkl, 'rb'))\n",
    "id_test, x_test,pre_x_test, L_test, y_test, sz_test, time_test, _ = pickle.load(open(cf.test_pkl, 'rb'))\n",
    "id_val, x_val,pre_x_val, L_val, y_val, sz_val, time_val, _ = pickle.load(open(cf.val_pkl, 'rb'))\n",
    "for i in range(len(x_train)):\n",
    "    x_train[i] = np.array(x_train[i])\n",
    "for i in range(len(x_val)):\n",
    "    x_val[i] = np.array(x_val[i])\n",
    "for i in range(len(x_test)):\n",
    "    x_test[i] = np.array(x_test[i])\n",
    "x_val = np.array(x_val)\n",
    "pre_x_val = np.array(pre_x_val)\n",
    "x_train = np.array(x_train)\n",
    "pre_x_train = np.array(pre_x_train)\n",
    "x_test = np.array(x_test)\n",
    "pre_x_test = np.array(pre_x_test)\n",
    "\n",
    "y_val = np.array(y_val)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "print(x_train.shape)\n",
    "print(pre_x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(pre_x_test.shape)\n",
    "print(x_val.shape)\n",
    "print(pre_x_val.shape)\n",
    "print(y_train.shape)\n",
    "# x_train = x_train.reshape(x_train.shape[0], TIME_STEPS, INPUT_SIZE, 1)\n",
    "# x_test = x_test.reshape(x_test.shape[0], TIME_STEPS, INPUT_SIZE, 1)\n",
    "# x_val = x_val.reshape(x_val.shape[0], TIME_STEPS, INPUT_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        Z_Score(pre_x_train[:,i,j])\n",
    "        Z_Score(pre_x_test[:,i,j])\n",
    "        Z_Score(pre_x_val[:,i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, 10)          0         \n",
      "_________________________________________________________________\n",
      "gru1 (GRU)                   (None, None, 40)          6120      \n",
      "_________________________________________________________________\n",
      "gru2 (GRU)                   (None, None, 20)          3660      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 10)          210       \n",
      "_________________________________________________________________\n",
      "de_gru1 (GRU)                (None, None, 20)          1860      \n",
      "_________________________________________________________________\n",
      "de_gru2 (GRU)                (None, None, 40)          7320      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, None, 6)           246       \n",
      "=================================================================\n",
      "Total params: 19,416\n",
      "Trainable params: 19,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/20\n",
      "29529/29529 [==============================] - 33s 1ms/step - loss: 0.7625 - mse: 0.7625 - val_loss: 0.5868 - val_mse: 0.5868\n",
      "Epoch 2/20\n",
      "  192/29529 [..............................] - ETA: 24s - loss: 1.2641 - mse: 1.2641"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\keras\\callbacks\\callbacks.py:1042: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_mse,loss,mse,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29529/29529 [==============================] - 26s 877us/step - loss: 0.5380 - mse: 0.5380 - val_loss: 0.5063 - val_mse: 0.5063\n",
      "Epoch 3/20\n",
      "29529/29529 [==============================] - 27s 921us/step - loss: 0.4819 - mse: 0.4819 - val_loss: 0.4768 - val_mse: 0.4768\n",
      "Epoch 4/20\n",
      "29529/29529 [==============================] - 28s 955us/step - loss: 0.4549 - mse: 0.4549 - val_loss: 0.4409 - val_mse: 0.4409\n",
      "Epoch 5/20\n",
      "29529/29529 [==============================] - 27s 923us/step - loss: 0.4337 - mse: 0.4337 - val_loss: 0.4212 - val_mse: 0.4212\n",
      "Epoch 6/20\n",
      "29529/29529 [==============================] - 29s 989us/step - loss: 0.4193 - mse: 0.4193 - val_loss: 0.4168 - val_mse: 0.4168\n",
      "Epoch 7/20\n",
      "29529/29529 [==============================] - 28s 934us/step - loss: 0.4073 - mse: 0.4073 - val_loss: 0.4106 - val_mse: 0.4106\n",
      "Epoch 8/20\n",
      "29529/29529 [==============================] - 36s 1ms/step - loss: 0.4005 - mse: 0.4005 - val_loss: 0.3843 - val_mse: 0.3843\n",
      "Epoch 9/20\n",
      "29529/29529 [==============================] - 30s 1ms/step - loss: 0.3899 - mse: 0.3899 - val_loss: 0.3931 - val_mse: 0.3931\n",
      "Epoch 10/20\n",
      "29529/29529 [==============================] - 29s 977us/step - loss: 0.3831 - mse: 0.3831 - val_loss: 0.3713 - val_mse: 0.3713\n",
      "Epoch 11/20\n",
      "29529/29529 [==============================] - 29s 981us/step - loss: 0.3801 - mse: 0.3801 - val_loss: 0.3826 - val_mse: 0.3826\n",
      "Epoch 12/20\n",
      "29529/29529 [==============================] - 30s 1ms/step - loss: 0.3754 - mse: 0.3754 - val_loss: 0.3834 - val_mse: 0.3834\n",
      "Epoch 13/20\n",
      "29529/29529 [==============================] - 31s 1ms/step - loss: 0.3672 - mse: 0.3672 - val_loss: 0.3730 - val_mse: 0.3730\n",
      "Epoch 14/20\n",
      "29529/29529 [==============================] - 33s 1ms/step - loss: 0.3732 - mse: 0.3732 - val_loss: 0.3635 - val_mse: 0.3635\n",
      "Epoch 15/20\n",
      "29529/29529 [==============================] - 33s 1ms/step - loss: 0.3637 - mse: 0.3637 - val_loss: 0.3747 - val_mse: 0.3747\n",
      "Epoch 16/20\n",
      "29529/29529 [==============================] - 32s 1ms/step - loss: 0.3623 - mse: 0.3623 - val_loss: 0.3729 - val_mse: 0.3729\n",
      "Epoch 17/20\n",
      "29529/29529 [==============================] - 33s 1ms/step - loss: 0.3568 - mse: 0.3568 - val_loss: 0.3762 - val_mse: 0.3762\n",
      "Epoch 18/20\n",
      "29529/29529 [==============================] - 33s 1ms/step - loss: 0.3561 - mse: 0.3561 - val_loss: 0.3592 - val_mse: 0.3592\n",
      "Epoch 19/20\n",
      "29529/29529 [==============================] - 31s 1ms/step - loss: 0.3528 - mse: 0.3528 - val_loss: 0.3732 - val_mse: 0.3732\n",
      "Epoch 20/20\n",
      "29529/29529 [==============================] - 37s 1ms/step - loss: 0.3503 - mse: 0.3503 - val_loss: 0.3677 - val_mse: 0.3677\n"
     ]
    }
   ],
   "source": [
    "## pre autoencoder\n",
    "\n",
    "retrunSeq=True\n",
    "input_data = Input((None,INPUT_SIZE))\n",
    "encoder=GRU(units=OUTPUT_SIZE*2,  activation='tanh',return_sequences=retrunSeq, name=\"gru1\")(input_data)\n",
    "encoder=GRU(units=OUTPUT_SIZE,  activation='tanh',return_sequences=retrunSeq, name=\"gru2\")(encoder)\n",
    "encoder_out=Dense(INPUT_SIZE,activation='tanh')(encoder)\n",
    "encoder_model = Model(inputs=input_data, outputs=encoder_out)\n",
    "\n",
    "decoder=GRU(units=OUTPUT_SIZE, activation='tanh',return_sequences=retrunSeq,name=\"de_gru1\")(encoder_out)\n",
    "decoder=GRU(units=OUTPUT_SIZE*2, activation='tanh', return_sequences=retrunSeq, name=\"de_gru2\")(decoder)\n",
    "decoder_out=Dense(PRE_INPUT_SIZE,activation='relu')(decoder)\n",
    "autoencoder=Model(input_data,decoder_out)\n",
    " \n",
    "#Compile\n",
    "#autoencoder = multi_gpu_model(autoencoder, 1)\n",
    "autoencoder.compile(loss='mean_squared_error',optimizer='adadelta',metrics=['mse'])\n",
    "autoencoder.summary()  #Print\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_acc', patience = 3,verbose = 1, factor=0.5, min_lr = 0.000001)\n",
    "#Train\n",
    "autoencoder.fit(x_train, pre_x_train, batch_size=batch_size, epochs=epochs, validation_data=(x_val,pre_x_val), \n",
    "          shuffle=True, verbose=1,callbacks=[learning_rate_reduction])\n",
    "\n",
    "#encoderModel Migration\n",
    "\n",
    "x_train_mid = encoder_model.predict(x_train)\n",
    "x_test_mid = encoder_model.predict(x_test)\n",
    "x_val_mid = encoder_model.predict(x_val)\n",
    "\n",
    "#x_train = np.concatenate((x_train,x_train_plus),axis=2)\n",
    "#x_test =np.concatenate((x_test,x_test_plus),axis=2)\n",
    "#x_val = np.concatenate((x_val,x_val_plus),axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 6, 20)             1860      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 6, 20)             2460      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                1936      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 6,273\n",
      "Trainable params: 6,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 16s 536us/step - loss: 3.0562 - mse: 3.0562 - val_loss: 2.4888 - val_mse: 2.4888  - ETA: 1s - loss: 3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\keras\\callbacks\\callbacks.py:1042: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_mse,loss,mse,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre Test score: 2.3851101409396005\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "  288/29529 [..............................] - ETA: 12s - loss: 2.4449 - mse: 2.44"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29529/29529 [==============================] - 14s 481us/step - loss: 2.2554 - mse: 2.2554 - val_loss: 2.3373 - val_mse: 2.3373\n",
      "Pre Test score: 2.192839645031826\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 17s 568us/step - loss: 2.2039 - mse: 2.2039 - val_loss: 2.4668 - val_mse: 2.4668\n",
      "Pre Test score: 2.2975569067548496\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 15s 511us/step - loss: 2.1799 - mse: 2.1799 - val_loss: 2.2841 - val_mse: 2.2841\n",
      "Pre Test score: 2.1506509771463107\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 14s 462us/step - loss: 2.1622 - mse: 2.1622 - val_loss: 2.3434 - val_mse: 2.3434\n",
      "Pre Test score: 2.1911669859336755\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 14s 472us/step - loss: 2.1494 - mse: 2.1494 - val_loss: 2.2548 - val_mse: 2.2548\n",
      "Pre Test score: 2.133565229658872\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 14s 463us/step - loss: 2.1369 - mse: 2.1369 - val_loss: 2.2521 - val_mse: 2.2521\n",
      "Pre Test score: 2.1227683227397978\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 15s 521us/step - loss: 2.1293 - mse: 2.1293 - val_loss: 2.3387 - val_mse: 2.3387\n",
      "Pre Test score: 2.184431730862457\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 14s 476us/step - loss: 2.1196 - mse: 2.1196 - val_loss: 2.2435 - val_mse: 2.2435\n",
      "Pre Test score: 2.134292539797331\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 14s 476us/step - loss: 2.1092 - mse: 2.1092 - val_loss: 2.3212 - val_mse: 2.3212\n",
      "Pre Test score: 2.1904532267656562\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 15s 492us/step - loss: 2.1042 - mse: 2.1042 - val_loss: 2.3963 - val_mse: 2.3963\n",
      "Pre Test score: 2.33604232006697\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 14s 485us/step - loss: 2.0943 - mse: 2.0943 - val_loss: 2.2239 - val_mse: 2.2239\n",
      "Pre Test score: 2.097896118987053\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 14s 477us/step - loss: 2.0919 - mse: 2.0919 - val_loss: 2.2523 - val_mse: 2.2523\n",
      "Pre Test score: 2.120156388646791\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 14s 485us/step - loss: 2.0840 - mse: 2.0840 - val_loss: 2.2036 - val_mse: 2.2036\n",
      "Pre Test score: 2.0836638500288385\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 14s 472us/step - loss: 2.0791 - mse: 2.0791 - val_loss: 2.2107 - val_mse: 2.2107\n",
      "Pre Test score: 2.1053376979731415\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 15s 501us/step - loss: 2.0737 - mse: 2.0737 - val_loss: 2.1866 - val_mse: 2.18664s - loss: 2.\n",
      "Pre Test score: 2.0812553432441088\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 14s 472us/step - loss: 2.0677 - mse: 2.0677 - val_loss: 2.2055 - val_mse: 2.2055\n",
      "Pre Test score: 2.1106019644106433\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 14s 477us/step - loss: 2.0619 - mse: 2.0619 - val_loss: 2.2004 - val_mse: 2.2004\n",
      "Pre Test score: 2.1053530285101054\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 15s 497us/step - loss: 2.0642 - mse: 2.0642 - val_loss: 2.2067 - val_mse: 2.2067\n",
      "Pre Test score: 2.0901958838785846\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 14s 467us/step - loss: 2.0557 - mse: 2.0557 - val_loss: 2.2587 - val_mse: 2.2587\n",
      "Pre Test score: 2.1227098942481266\n"
     ]
    }
   ],
   "source": [
    "## LSTM \n",
    "model = Sequential()\n",
    "model.add(GRU(units = OUTPUT_SIZE, activation='tanh', input_shape=input_shape,return_sequences=True))\n",
    "model.add(GRU(units = OUTPUT_SIZE, activation='tanh',return_sequences=True))\n",
    "model.add(Flatten()) #Pull into one-dimensional data\n",
    "model.add(Dense(16, activation='relu')) \n",
    "model.add(Dense(1, activation='relu')) #Fully connected layer\n",
    "# model.add(Dense(1, activation='relu')) #Fully connected layer\n",
    "\n",
    "#Compile\n",
    "#model = multi_gpu_model(model, 1)\n",
    "model.compile(loss='mean_squared_error',optimizer='adadelta',metrics=['mse'])\n",
    "model.summary()  #Print\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_acc', patience = 3,verbose = 1, factor=0.5, min_lr = 0.000000001)\n",
    "#Train\n",
    "f = open(datasetName+'result_epoch_rnn.csv','w')\n",
    "for epoch in range(epochs):\n",
    "    model.fit(x_train_mid, y_train, batch_size=batch_size, nb_epoch=1, validation_data=(x_val_mid, y_val), \n",
    "              shuffle=True, verbose=1,callbacks=[learning_rate_reduction])\n",
    "    pre_test_score = model.evaluate(x_test_mid, y_test, verbose=0)\n",
    "    pre_val_score  = model.evaluate(x_val_mid, y_val, verbose=0)\n",
    "    f.write(str(epoch)+',Pre Test score,'+ str(pre_test_score[0])+',Pre Val_score,'+str(pre_val_score[0])+'\\n')\n",
    "    print('Pre Test score:', pre_test_score[0])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, None, 10)          0         \n",
      "_________________________________________________________________\n",
      "gru1 (GRU)                   (None, None, 40)          6120      \n",
      "_________________________________________________________________\n",
      "gru2 (GRU)                   (None, None, 20)          3660      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, None, 20)          420       \n",
      "_________________________________________________________________\n",
      "de_gru1 (GRU)                (None, None, 20)          2460      \n",
      "_________________________________________________________________\n",
      "de_gru2 (GRU)                (None, None, 40)          7320      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, None, 10)          410       \n",
      "=================================================================\n",
      "Total params: 20,390\n",
      "Trainable params: 20,390\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/20\n",
      "29529/29529 [==============================] - 30s 1ms/step - loss: 0.7007 - mse: 0.7007 - val_loss: 0.6976 - val_mse: 0.6976\n",
      "Epoch 2/20\n",
      "  160/29529 [..............................] - ETA: 26s - loss: 0.6956 - mse: 0.6956"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\keras\\callbacks\\callbacks.py:1042: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_mse,loss,mse,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29529/29529 [==============================] - 29s 988us/step - loss: 0.6982 - mse: 0.6982 - val_loss: 0.6975 - val_mse: 0.6975\n",
      "Epoch 3/20\n",
      "29529/29529 [==============================] - 27s 921us/step - loss: 0.6982 - mse: 0.6982 - val_loss: 0.6974 - val_mse: 0.6974\n",
      "Epoch 4/20\n",
      "29529/29529 [==============================] - 29s 968us/step - loss: 0.6982 - mse: 0.6982 - val_loss: 0.6974 - val_mse: 0.6974\n",
      "Epoch 5/20\n",
      "29529/29529 [==============================] - 27s 898us/step - loss: 0.6981 - mse: 0.6981 - val_loss: 0.6974 - val_mse: 0.6974\n",
      "Epoch 6/20\n",
      "29529/29529 [==============================] - 26s 880us/step - loss: 0.6981 - mse: 0.6981 - val_loss: 0.6974 - val_mse: 0.6974\n",
      "Epoch 7/20\n",
      "29529/29529 [==============================] - 26s 894us/step - loss: 0.6981 - mse: 0.6981 - val_loss: 0.6974 - val_mse: 0.6974\n",
      "Epoch 8/20\n",
      "29529/29529 [==============================] - 29s 993us/step - loss: 0.6981 - mse: 0.6981 - val_loss: 0.6974 - val_mse: 0.6974\n",
      "Epoch 9/20\n",
      "29529/29529 [==============================] - 28s 955us/step - loss: 0.6981 - mse: 0.6981 - val_loss: 0.6973 - val_mse: 0.6973\n",
      "Epoch 10/20\n",
      "29529/29529 [==============================] - 30s 1ms/step - loss: 0.6981 - mse: 0.6981 - val_loss: 0.6973 - val_mse: 0.6973\n",
      "Epoch 11/20\n",
      "29529/29529 [==============================] - 32s 1ms/step - loss: 0.6981 - mse: 0.6981 - val_loss: 0.6973 - val_mse: 0.6973\n",
      "Epoch 12/20\n",
      "29529/29529 [==============================] - 32s 1ms/step - loss: 0.6980 - mse: 0.6980 - val_loss: 0.6972 - val_mse: 0.6972\n",
      "Epoch 13/20\n",
      "29529/29529 [==============================] - 27s 911us/step - loss: 0.6979 - mse: 0.6979 - val_loss: 0.6969 - val_mse: 0.6969\n",
      "Epoch 14/20\n",
      "29529/29529 [==============================] - 27s 928us/step - loss: 0.6977 - mse: 0.6977 - val_loss: 0.6968 - val_mse: 0.6968\n",
      "Epoch 15/20\n",
      "29529/29529 [==============================] - 26s 891us/step - loss: 0.6976 - mse: 0.6976 - val_loss: 0.6967 - val_mse: 0.6967\n",
      "Epoch 16/20\n",
      "29529/29529 [==============================] - 26s 897us/step - loss: 0.6975 - mse: 0.6975 - val_loss: 0.6967 - val_mse: 0.6967\n",
      "Epoch 17/20\n",
      "29529/29529 [==============================] - 27s 912us/step - loss: 0.6975 - mse: 0.6975 - val_loss: 0.6967 - val_mse: 0.6967\n",
      "Epoch 18/20\n",
      "29529/29529 [==============================] - 27s 903us/step - loss: 0.6975 - mse: 0.6975 - val_loss: 0.6967 - val_mse: 0.6967\n",
      "Epoch 19/20\n",
      "29529/29529 [==============================] - 27s 929us/step - loss: 0.6975 - mse: 0.6975 - val_loss: 0.6966 - val_mse: 0.6966\n",
      "Epoch 20/20\n",
      "29529/29529 [==============================] - 26s 893us/step - loss: 0.6975 - mse: 0.6975 - val_loss: 0.6966 - val_mse: 0.6966 - loss: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:35: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:46: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                1936      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,953\n",
      "Trainable params: 1,953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 74us/step - loss: 3.6381 - mse: 3.6381 - val_loss: 2.9961 - val_mse: 2.9961\n",
      "Pre Test score: 2.9513669614639633\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 57us/step - loss: 2.6590 - mse: 2.6590 - val_loss: 2.6720 - val_mse: 2.6720\n",
      "Pre Test score: 2.573175089231539\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 68us/step - loss: 2.5851 - mse: 2.5851 - val_loss: 2.7204 - val_mse: 2.7204\n",
      "Pre Test score: 2.6325132936140325\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 77us/step - loss: 2.5437 - mse: 2.5437 - val_loss: 2.6098 - val_mse: 2.6098\n",
      "Pre Test score: 2.4908209256657083\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 63us/step - loss: 2.5203 - mse: 2.5203 - val_loss: 2.5901 - val_mse: 2.5901\n",
      "Pre Test score: 2.4655781333737035\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 61us/step - loss: 2.5030 - mse: 2.5030 - val_loss: 2.5843 - val_mse: 2.5843\n",
      "Pre Test score: 2.460069977014348\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 60us/step - loss: 2.4934 - mse: 2.4934 - val_loss: 2.5701 - val_mse: 2.5701\n",
      "Pre Test score: 2.4426831258912776\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 59us/step - loss: 2.4674 - mse: 2.4674 - val_loss: 2.5468 - val_mse: 2.5468\n",
      "Pre Test score: 2.4287473645201976\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 75us/step - loss: 2.4581 - mse: 2.4581 - val_loss: 2.5322 - val_mse: 2.5322\n",
      "Pre Test score: 2.404267626990446\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 74us/step - loss: 2.4346 - mse: 2.4346 - val_loss: 2.5515 - val_mse: 2.5515\n",
      "Pre Test score: 2.4416037490625264\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 61us/step - loss: 2.4200 - mse: 2.4200 - val_loss: 2.5213 - val_mse: 2.5213\n",
      "Pre Test score: 2.394930318951776\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 60us/step - loss: 2.4132 - mse: 2.4132 - val_loss: 2.5272 - val_mse: 2.5272\n",
      "Pre Test score: 2.4201068145998548\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 60us/step - loss: 2.4046 - mse: 2.4046 - val_loss: 2.5041 - val_mse: 2.5041\n",
      "Pre Test score: 2.3816468932720203\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 71us/step - loss: 2.3941 - mse: 2.3941 - val_loss: 2.5137 - val_mse: 2.5137\n",
      "Pre Test score: 2.370879579573912\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 62us/step - loss: 2.3942 - mse: 2.3942 - val_loss: 2.8143 - val_mse: 2.8143\n",
      "Pre Test score: 2.6355470267633927\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 62us/step - loss: 2.3823 - mse: 2.3823 - val_loss: 2.4593 - val_mse: 2.4593\n",
      "Pre Test score: 2.3369062999724135\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 60us/step - loss: 2.3776 - mse: 2.3776 - val_loss: 2.4687 - val_mse: 2.4687\n",
      "Pre Test score: 2.355117756513617\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 59us/step - loss: 2.3739 - mse: 2.3739 - val_loss: 2.5940 - val_mse: 2.5940\n",
      "Pre Test score: 2.5022037467893234\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 59us/step - loss: 2.3658 - mse: 2.3658 - val_loss: 2.4543 - val_mse: 2.4543\n",
      "Pre Test score: 2.322688150390811\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 62us/step - loss: 2.3639 - mse: 2.3639 - val_loss: 2.7305 - val_mse: 2.7305\n",
      "Pre Test score: 2.635932871680021\n"
     ]
    }
   ],
   "source": [
    "## LSTM autoencoder\n",
    "retrunSeq=True\n",
    "input_data = Input((None,INPUT_SIZE))\n",
    "encoder=GRU(units=OUTPUT_SIZE*2,  activation='tanh',return_sequences=retrunSeq, name=\"gru1\")(input_data)\n",
    "encoder=GRU(units=OUTPUT_SIZE,  activation='tanh',return_sequences=retrunSeq, name=\"gru2\")(encoder)\n",
    "encoder_out=Dense(OUTPUT_SIZE,activation='tanh')(encoder)\n",
    "encoder_model = Model(inputs=input_data, outputs=encoder_out)\n",
    "\n",
    "decoder=GRU(units=OUTPUT_SIZE, activation='tanh',return_sequences=retrunSeq,name=\"de_gru1\")(encoder_out)\n",
    "decoder=GRU(units=OUTPUT_SIZE*2,  activation='tanh', return_sequences=retrunSeq, name=\"de_gru2\")(decoder)\n",
    "decoder_out=Dense(INPUT_SIZE,activation='relu')(decoder)\n",
    "autoencoder=Model(input_data,decoder_out)\n",
    " \n",
    "#Compile\n",
    "#autoencoder = multi_gpu_model(autoencoder, 1)\n",
    "autoencoder.compile(loss='mean_squared_error',optimizer='adadelta',metrics=['mse'])\n",
    "autoencoder.summary()  #Print\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_acc', patience = 3,verbose = 1, factor=0.5, min_lr = 0.000001)\n",
    "#Train\n",
    "autoencoder.fit(x_train_mid, x_train_mid, batch_size=batch_size, epochs=epochs, validation_data=(x_val_mid, x_val_mid), \n",
    "          shuffle=True, verbose=1,callbacks=[learning_rate_reduction])\n",
    "#Predict\n",
    "encoded_latent_train = encoder_model.predict(x_train_mid)\n",
    "encoded_latent_test = encoder_model.predict(x_test_mid)\n",
    "encoded_latent_val = encoder_model.predict(x_val_mid)\n",
    "if retrunSeq:\n",
    "    encoded_latent_train=encoded_latent_train.reshape(-1,OUTPUT_SIZE*TIME_STEPS)\n",
    "    encoded_latent_test=encoded_latent_test.reshape(-1,OUTPUT_SIZE*TIME_STEPS)\n",
    "    encoded_latent_val=encoded_latent_val.reshape(-1,OUTPUT_SIZE*TIME_STEPS)\n",
    "\n",
    "input_representation = Input(shape=(OUTPUT_SIZE*TIME_STEPS,))\n",
    "latent_vector = Dense(16, activation='relu')(input_representation)\n",
    "preddiction = Dense(1, activation='relu')(latent_vector)\n",
    "model2 = Model(input= input_representation,output=preddiction)\n",
    "#Compile\n",
    "#model2 = multi_gpu_model(model2, 2)\n",
    "model2.compile(loss='mean_squared_error',optimizer='adadelta',metrics=['mse'])\n",
    "model2.summary()  #Print\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_acc', patience = 3,verbose = 1, factor=0.5, min_lr = 0.00001)\n",
    "#Train\n",
    "f = open('result_epoch_rnn_ae.csv','w')\n",
    "for epoch in range(epochs):\n",
    "    model2.fit(encoded_latent_train, y_train, batch_size=batch_size, nb_epoch=1, validation_data=(encoded_latent_val, y_val), \n",
    "              shuffle=True, verbose=1,callbacks=[learning_rate_reduction])\n",
    "    pre_test_score = model2.evaluate(encoded_latent_test, y_test, verbose=0)\n",
    "    pre_val_score  = model2.evaluate(encoded_latent_val, y_val, verbose=0)\n",
    "    f.write(str(epoch)+',Pre Test score,'+ str(pre_test_score[0])+',Pre Val_score,'+str(pre_val_score[0])+'\\n')\n",
    "    print('Pre Test score:', pre_test_score[0])\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 29s 983us/step - loss: 2.9125 - mse: 2.9125 - val_loss: 2.4290 - val_mse: 2.4290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\keras\\callbacks\\callbacks.py:1042: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_mse,loss,mse,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre Test score: 2.295050116121929\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "  160/29529 [..............................] - ETA: 27s - loss: 3.2097 - mse: 3.2097"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29529/29529 [==============================] - 28s 955us/step - loss: 2.2609 - mse: 2.2609 - val_loss: 2.3232 - val_mse: 2.3232\n",
      "Pre Test score: 2.2115785257876985\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 25s 843us/step - loss: 2.2132 - mse: 2.2132 - val_loss: 2.3648 - val_mse: 2.3648\n",
      "Pre Test score: 2.285798688550611\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 24s 822us/step - loss: 2.1775 - mse: 2.1775 - val_loss: 2.3355 - val_mse: 2.3355\n",
      "Pre Test score: 2.2081324314833704\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 24s 819us/step - loss: 2.1599 - mse: 2.1599 - val_loss: 2.2539 - val_mse: 2.2539\n",
      "Pre Test score: 2.133282559770006\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 25s 831us/step - loss: 2.1406 - mse: 2.1406 - val_loss: 2.3020 - val_mse: 2.3020\n",
      "Pre Test score: 2.2314795695606784\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 25s 840us/step - loss: 2.1319 - mse: 2.1319 - val_loss: 2.2436 - val_mse: 2.2436\n",
      "Pre Test score: 2.1257607127057447\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 26s 886us/step - loss: 2.1201 - mse: 2.1201 - val_loss: 2.2595 - val_mse: 2.2595\n",
      "Pre Test score: 2.1315294271098018\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 25s 831us/step - loss: 2.1180 - mse: 2.1180 - val_loss: 2.4200 - val_mse: 2.4200\n",
      "Pre Test score: 2.268078374998329\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 25s 841us/step - loss: 2.1084 - mse: 2.1084 - val_loss: 2.2354 - val_mse: 2.2354\n",
      "Pre Test score: 2.1319082686172686\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 30s 1ms/step - loss: 2.1021 - mse: 2.1021 - val_loss: 2.2463 - val_mse: 2.2463\n",
      "Pre Test score: 2.14586141110597\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 25s 859us/step - loss: 2.0994 - mse: 2.0994 - val_loss: 2.3298 - val_mse: 2.3298\n",
      "Pre Test score: 2.2055178704901635\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 24s 810us/step - loss: 2.0900 - mse: 2.0900 - val_loss: 2.2881 - val_mse: 2.2881\n",
      "Pre Test score: 2.154957945169347\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 24s 829us/step - loss: 2.0847 - mse: 2.0847 - val_loss: 2.2324 - val_mse: 2.2324\n",
      "Pre Test score: 2.1022174041079404\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 31s 1ms/step - loss: 2.0802 - mse: 2.0802 - val_loss: 2.2604 - val_mse: 2.2604\n",
      "Pre Test score: 2.1132167970585902\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 30s 1ms/step - loss: 2.0799 - mse: 2.0799 - val_loss: 2.1965 - val_mse: 2.1965\n",
      "Pre Test score: 2.0976100782280156\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 25s 843us/step - loss: 2.0789 - mse: 2.0789 - val_loss: 2.2289 - val_mse: 2.2289\n",
      "Pre Test score: 2.1373372591180564\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 25s 850us/step - loss: 2.0718 - mse: 2.0718 - val_loss: 2.1975 - val_mse: 2.1975\n",
      "Pre Test score: 2.083877425482436\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 26s 893us/step - loss: 2.0718 - mse: 2.0718 - val_loss: 2.2968 - val_mse: 2.2968\n",
      "Pre Test score: 2.146913460085926\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 26s 870us/step - loss: 2.0653 - mse: 2.0653 - val_loss: 2.2938 - val_mse: 2.2938\n",
      "Pre Test score: 2.1574765572111545\n"
     ]
    }
   ],
   "source": [
    "## LSTM SelfAttention\n",
    "\n",
    "model = Sequential()\n",
    "model.add(keras.layers.Bidirectional(keras.layers.GRU(units=OUTPUT_SIZE, input_shape=input_shape,return_sequences=True)))\n",
    "model.add(keras.layers.Bidirectional(keras.layers.GRU(units=OUTPUT_SIZE,return_sequences=True)))\n",
    "model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
    "#model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
    "model.add(Flatten()) #Pull into one-dimensional data\n",
    "model.add(Dense(16, activation='relu')) \n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(1, activation='relu')) #Fully connected layer\n",
    "# model.add(Dense(1, activation='relu')) #Fully connected layer\n",
    " \n",
    "#Compile\n",
    "#model = multi_gpu_model(model, 1)\n",
    "model.compile(loss='mean_squared_error',optimizer='adadelta',metrics=['mse'])\n",
    "#model.summary()  #Print\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_acc', patience = 3,verbose = 1, factor=0.5, min_lr = 0.0000004)\n",
    "#Train\n",
    "f = open('result_epoch_rnn_SA.csv','w')\n",
    "for epoch in range(epochs):\n",
    "    model.fit(x_train_mid, y_train, batch_size=batch_size, nb_epoch=1, validation_data=(x_val_mid, y_val), \n",
    "              shuffle=True, verbose=1,callbacks=[learning_rate_reduction])\n",
    "    pre_test_score = model.evaluate(x_test_mid, y_test, verbose=0)\n",
    "    pre_val_score  = model.evaluate(x_val_mid, y_val, verbose=0)\n",
    "    f.write(str(epoch)+',Pre Test score,'+ str(pre_test_score[0])+',Pre Val_score,'+str(pre_val_score[0])+'\\n')\n",
    "    print('Pre Test score:', pre_test_score[0])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 24s 800us/step - loss: 2.7622 - mse: 2.7622 - val_loss: 2.4184 - val_mse: 2.4184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\keras\\callbacks\\callbacks.py:1042: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_mse,loss,mse,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre Test score: 2.30385582726281\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "  192/29529 [..............................] - ETA: 20s - loss: 2.7906 - mse: 2.79"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29529/29529 [==============================] - 22s 734us/step - loss: 2.2525 - mse: 2.2525 - val_loss: 2.5514 - val_mse: 2.5514\n",
      "Pre Test score: 2.464182255372478\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 22s 760us/step - loss: 2.2077 - mse: 2.2077 - val_loss: 2.3562 - val_mse: 2.3562\n",
      "Pre Test score: 2.236393084774014\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 21s 725us/step - loss: 2.1792 - mse: 2.1792 - val_loss: 2.3287 - val_mse: 2.3287\n",
      "Pre Test score: 2.181010166653265\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 22s 756us/step - loss: 2.1601 - mse: 2.1601 - val_loss: 2.3491 - val_mse: 2.3491\n",
      "Pre Test score: 2.257734971719461\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 22s 753us/step - loss: 2.1472 - mse: 2.1472 - val_loss: 2.4286 - val_mse: 2.4286\n",
      "Pre Test score: 2.278921703751927\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 22s 749us/step - loss: 2.1285 - mse: 2.1285 - val_loss: 2.3325 - val_mse: 2.3325\n",
      "Pre Test score: 2.247858707008344\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 22s 734us/step - loss: 2.1232 - mse: 2.1232 - val_loss: 2.3046 - val_mse: 2.3046\n",
      "Pre Test score: 2.216402146796282\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 23s 770us/step - loss: 2.1117 - mse: 2.1117 - val_loss: 2.2657 - val_mse: 2.2657\n",
      "Pre Test score: 2.140947853348842\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 23s 785us/step - loss: 2.1056 - mse: 2.1056 - val_loss: 2.2458 - val_mse: 2.2458\n",
      "Pre Test score: 2.1315281693736052\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 24s 797us/step - loss: 2.1024 - mse: 2.1024 - val_loss: 2.2876 - val_mse: 2.2876\n",
      "Pre Test score: 2.1537677823828356\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 22s 739us/step - loss: 2.0940 - mse: 2.0940 - val_loss: 2.2588 - val_mse: 2.2588\n",
      "Pre Test score: 2.166130238165689\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 22s 739us/step - loss: 2.0928 - mse: 2.0928 - val_loss: 2.2611 - val_mse: 2.2611\n",
      "Pre Test score: 2.1744954981320803\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 24s 799us/step - loss: 2.0843 - mse: 2.0843 - val_loss: 2.2558 - val_mse: 2.2558\n",
      "Pre Test score: 2.119695912761034\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 22s 742us/step - loss: 2.0825 - mse: 2.0825 - val_loss: 2.2286 - val_mse: 2.2286\n",
      "Pre Test score: 2.1101887479332624\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 22s 755us/step - loss: 2.0797 - mse: 2.0797 - val_loss: 2.2573 - val_mse: 2.2573ss\n",
      "Pre Test score: 2.1267889811752605\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 22s 745us/step - loss: 2.0675 - mse: 2.0675 - val_loss: 2.2008 - val_mse: 2.2008\n",
      "Pre Test score: 2.092323049542575\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 22s 753us/step - loss: 2.0659 - mse: 2.0659 - val_loss: 2.2555 - val_mse: 2.2555\n",
      "Pre Test score: 2.1828576878610524\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 20s 678us/step - loss: 2.0618 - mse: 2.0618 - val_loss: 2.2349 - val_mse: 2.2349\n",
      "Pre Test score: 2.1285725962556103\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 21s 718us/step - loss: 2.0628 - mse: 2.0628 - val_loss: 2.2026 - val_mse: 2.2026\n",
      "Pre Test score: 2.0792447915099066\n"
     ]
    }
   ],
   "source": [
    "## LSTM Bi\n",
    "model = Sequential()\n",
    "#model.add(keras.layers.Embedding(input_dim=INPUT_SIZE, output_dim=16,mask_zero=True))\n",
    "model.add(keras.layers.Bidirectional(keras.layers.GRU(units=OUTPUT_SIZE, input_shape=input_shape,return_sequences=True)))\n",
    "model.add(keras.layers.Bidirectional(keras.layers.GRU(units=OUTPUT_SIZE,return_sequences=True)))\n",
    "\n",
    "model.add(Flatten()) #Pull into one-dimensional data\n",
    "model.add(Dense(16, activation='relu')) \n",
    "model.add(Dense(1, activation='relu')) #Fully connected layer\n",
    "# model.add(Dense(1, activation='relu')) #Fully connected layer\n",
    " \n",
    "#Compile\n",
    "#model = multi_gpu_model(model, 1)\n",
    "model.compile(loss='mean_squared_error',optimizer='adadelta',metrics=['mse'])\n",
    "#model.summary()  #Print\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_acc', patience = 3,verbose = 1, factor=0.5, min_lr = 0.00000001)\n",
    "#Train\n",
    "f = open(datasetName+'result_epoch_rnn_bi.csv','w')\n",
    "for epoch in range(epochs):\n",
    "    model.fit(x_train_mid, y_train, batch_size=batch_size, nb_epoch=1, validation_data=(x_val_mid, y_val), \n",
    "              shuffle=True, verbose=1,callbacks=[learning_rate_reduction])\n",
    "    pre_test_score = model.evaluate(x_test_mid, y_test, verbose=0)\n",
    "    pre_val_score  = model.evaluate(x_val_mid, y_val, verbose=0)\n",
    "    f.write(str(epoch)+',Pre Test score,'+ str(pre_test_score[0])+',Pre Val_score,'+str(pre_val_score[0])+'\\n')\n",
    "    print('Pre Test score:', pre_test_score[0])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 82us/step - loss: 5.1188 - mse: 5.1188 - val_loss: 2.6868 - val_mse: 2.6868\n",
      "Pre Test score: 2.5223091138149676\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 63us/step - loss: 2.2804 - mse: 2.2804 - val_loss: 2.3482 - val_mse: 2.3482\n",
      "Pre Test score: 2.243026273933014\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 66us/step - loss: 2.2281 - mse: 2.2281 - val_loss: 2.3577 - val_mse: 2.3577\n",
      "Pre Test score: 2.2294891702444875\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 67us/step - loss: 2.2069 - mse: 2.2069 - val_loss: 2.3390 - val_mse: 2.3390\n",
      "Pre Test score: 2.208124977087398\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 67us/step - loss: 2.1912 - mse: 2.1912 - val_loss: 2.3297 - val_mse: 2.3297\n",
      "Pre Test score: 2.1982829934785353\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 66us/step - loss: 2.1819 - mse: 2.1819 - val_loss: 2.3963 - val_mse: 2.3963\n",
      "Pre Test score: 2.30624559880132\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 73us/step - loss: 2.1672 - mse: 2.1672 - val_loss: 2.2828 - val_mse: 2.2828\n",
      "Pre Test score: 2.16459095645746\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 65us/step - loss: 2.1585 - mse: 2.1585 - val_loss: 2.2918 - val_mse: 2.2918\n",
      "Pre Test score: 2.1636771233665524\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 64us/step - loss: 2.1517 - mse: 2.1517 - val_loss: 2.3434 - val_mse: 2.3434\n",
      "Pre Test score: 2.2014297270861514\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 63us/step - loss: 2.1486 - mse: 2.1486 - val_loss: 2.3108 - val_mse: 2.3108\n",
      "Pre Test score: 2.2134323118609878\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 63us/step - loss: 2.1440 - mse: 2.1440 - val_loss: 2.3563 - val_mse: 2.3563\n",
      "Pre Test score: 2.262255165462756\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 63us/step - loss: 2.1399 - mse: 2.1399 - val_loss: 2.2692 - val_mse: 2.2692\n",
      "Pre Test score: 2.139994301049088\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 3s 111us/step - loss: 2.1350 - mse: 2.1350 - val_loss: 2.3444 - val_mse: 2.3444\n",
      "Pre Test score: 2.2602125754642017\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 84us/step - loss: 2.1292 - mse: 2.1292 - val_loss: 2.2634 - val_mse: 2.2634\n",
      "Pre Test score: 2.142633705582309\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 69us/step - loss: 2.1231 - mse: 2.1231 - val_loss: 2.2805 - val_mse: 2.2805\n",
      "Pre Test score: 2.1545918206760097\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 73us/step - loss: 2.1224 - mse: 2.1224 - val_loss: 2.3559 - val_mse: 2.3559\n",
      "Pre Test score: 2.210178979147436\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 70us/step - loss: 2.1242 - mse: 2.1242 - val_loss: 2.2803 - val_mse: 2.2803\n",
      "Pre Test score: 2.143393240778371\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 72us/step - loss: 2.1217 - mse: 2.1217 - val_loss: 2.4040 - val_mse: 2.4040\n",
      "Pre Test score: 2.247421346457626\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 73us/step - loss: 2.1194 - mse: 2.1194 - val_loss: 2.2820 - val_mse: 2.2820\n",
      "Pre Test score: 2.156056547496525\n",
      "Train on 29529 samples, validate on 6327 samples\n",
      "Epoch 1/1\n",
      "29529/29529 [==============================] - 2s 75us/step - loss: 2.1098 - mse: 2.1098 - val_loss: 2.2620 - val_mse: 2.2620\n",
      "Pre Test score: 2.124575039438676\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                1952      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,225\n",
      "Trainable params: 2,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## mlp\n",
    "model = Sequential()\n",
    "model.add(Flatten()) #Pull into one-dimensional data\n",
    "model.add(Dense(32, activation='relu')) \n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='relu')) #Fully connected layer\n",
    "# model.add(Dense(1, activation='relu')) #Fully connected layer\n",
    " \n",
    "#Compile\n",
    "#model = multi_gpu_model(model, 1)\n",
    "model.compile(loss='mean_squared_error',optimizer='adadelta',metrics=['mse'])\n",
    "#model.summary()  #Print\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_acc', patience = 3,verbose = 1, factor=0.5, min_lr = 0.00000001)\n",
    "#Train\n",
    "f = open(datasetName+'result_epoch_mlp.csv','w')\n",
    "for epoch in range(epochs):\n",
    "    model.fit(x_train_mid, y_train, batch_size=batch_size, nb_epoch=1, validation_data=(x_val_mid, y_val), \n",
    "              shuffle=True, verbose=1,callbacks=[learning_rate_reduction])\n",
    "    pre_test_score = model.evaluate(x_test_mid, y_test, verbose=0)\n",
    "    pre_val_score  = model.evaluate(x_val_mid, y_val, verbose=0)\n",
    "    f.write(str(epoch)+',Pre Test score,'+ str(pre_test_score[0])+',Pre Val_score,'+str(pre_val_score[0])+'\\n')\n",
    "    print('Pre Test score:', pre_test_score[0])\n",
    "f.close()\n",
    "model.summary() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
